{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop at DAGA 2024\n",
    "\n",
    "The **Py**thon packages **f**or **A**coustics **R**esearch (pyfar) aim at providing methods and functionality for acoustics research in a unified and comprehensive framework.\n",
    "Pyfar is developed with a strong focus on:\n",
    "\n",
    "- **Online documentation**.\n",
    "- **Automated testing**.\n",
    "- Unrestricted use through the liberal MIT or CC-BY **open source** licenses.\n",
    "\n",
    "The **pyfar base package** contains classes and functions for the acquisition, inspection, and processing of audio signals.\n",
    "For ease of maintainability and robustness to future changes, applied or specific features are split into sub-packages which extend the base package. These currently include:\n",
    "\n",
    "- **sofar**: SOFA file reader and writer functionality.\n",
    "- **spharpy**: Spherical array signal processing.\n",
    "- **pyrato**: Tools for room acoustic analysis.\n",
    "\n",
    "Please refer to [pyfar.org](https://pyfar.org) for an up-to-date list of packages and their intended use cases.\n",
    "\n",
    "We develop all packages openly across diverse research institutions, avoiding general branding of code or functionality. Contributions (in any form, i.e. reporting issues, submitting bug-fixes or implementation of new features) are welcome. \n",
    "To get in touch, either contact us via\n",
    "\n",
    "- [GitHub](https://github.com/pyfar).\n",
    "- [Join the Slack channel](https://join.slack.com/t/pyfar/shared_invite/zt-2eacdhww2-iUiPnh_wuqg2zD939wL4kw).\n",
    "- Write an old-school e-mail to info@pyfar.org."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Gallery\n",
    "\n",
    "This notebook, among others, is part of [pyfar's example gallery](https://pyfar-gallery.readthedocs.io/en/latest/).\n",
    "The gallery contains:\n",
    "\n",
    "- Notebooks supplementary to the [documentation](https://pyfar.readthedocs.io/en/latest).\n",
    "- Application examples giving an overview over the use cases pyfar and its sub-packages.\n",
    "\n",
    "All examples can either be \n",
    "\n",
    "- viewed as statically rendered web-pages,\n",
    "- interactively executed on mybinder.org,\n",
    "- downloaded and run locally.\n",
    "\n",
    "The respective links are located at the top of each example notebook. \n",
    "\n",
    "To follow along with the workshop document, either navigate to [pyfar.org](https://pyfar.org) and scroll down to the **Workshop DAGA 2024** example, or scan the following QR code:\n",
    "\n",
    "`I'm a little happy placeholder. When I'm grown up I'll be a QR code.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Disclaimer_:\n",
    "Please note that this is not a Python tutorial. We assume that you are aware of basic Python coding and concepts. For installations or set-up instructions please refer to the [getting started section of the documentation](https://pyfar.readthedocs.io/en/latest/readme.html#getting-started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pyfar as pf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set matplotlib backend for plotting\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Measurement Based Equalization Filter\n",
    "\n",
    "In the following, the creation of a measurement based equalization filter is used as an example of showing pyfar's functionality, namely\n",
    "\n",
    "- [Audio data and signals](#audio-data)\n",
    "- [Plotting](#interactive-plotting)\n",
    "- [Import and export](#import/export)\n",
    "- [Digital signal processing functions](#digital-signal-processing)\n",
    "- [Filter classes](#filter-classes)\n",
    "- [Example files](#example-files).\n",
    "\n",
    "Subsequently, the example of a [head-related impulse response](#example:-hrtf) will briefly introduce\n",
    "\n",
    "- [Coordinates](#coordinates)\n",
    "- [2D Plots](#2d-plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Data\n",
    "\n",
    "Audio data are the basis of pyfar and most data will be stored as `Signal` objects. Signals are intended for equally sampled time and frequency data. Examples for this are audio recordings and impulse responses, or corresponding spectra. Similarly, `TimeData` and `FrequencyData` objects are intended to store incomplete audio data, such as (third)-octave data or results from numerical simulations. A more detailed introduction to pyfar audio objects can be found [here](https://pyfar-gallery.readthedocs.io/en/latest/gallery/interactive/pyfar_audio_objects.html). \n",
    "\n",
    "Signals can be created in different ways.\n",
    "One possibility is to use the [pyfar.signals](https://pyfar.readthedocs.io/en/stable/modules/pyfar.signals.html) module. It contains functions for creating common signals like impulses, sine signals, sweeps, and noise. Let's create an impulse with a length of 1000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impulse = pf.signals.impulse(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying time and frequency data of the `Signal` object are stored as numpy arrays. We can access the time data with the `time` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impulse.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the frequency data with the `freq` attribute will automatically compute the spectrum using the Fast Fourier Transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impulse.freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Plotting\n",
    "\n",
    "Let's have a look at the time signal and spectrum by using the [pyfar.plot](https://pyfar.readthedocs.io/en/stable/modules/pyfar.plot.html) module. The plot functions are based on matplotlib, but adjusted for a convenient use in acoustics (e.g., logarithmic frequency axis, magnitudes in dB...). Also, pyfar provides convenient **interactive keyboard shortcuts**. These allow switching plots, zooming in and out, moving along the x and y-axis, and zooming and moving the range of colormaps\n",
    "\n",
    "To do this, you need to use an interactive matplotlib backend. For jupyter notebooks one can for example use the matplotlib magic `%matplotlib ipympl`, which has been executed at the top of the notebook already.\n",
    "\n",
    "A list of available shortcuts is found in the [documentation](https://pyfar.readthedocs.io/en/stable/concepts/pyfar.plot.html#interactive-plots) or by executing the following function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortcuts = pf.plot.shortcuts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, convenient default plotstyles are provided. To activate them for the upcoming code, just run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.plot.use()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All following figures can be used as interactive examples.** When the plot does not respond to the shortcuts, try clicking the figure to activate these for the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "pf.plot.time(impulse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import/Export\n",
    "\n",
    "Signals are often stored on disk. The [pyfar.io](https://pyfar.readthedocs.io/en/stable/modules/pyfar.io.html) module can read and write common audio file types such as *wav* and *mp3*. In addition, it can also import *sofa* files and data simulated with Comsol Multiphysics, and it allows saving and reading the workspace data including pyfar objects.\n",
    "\n",
    "This is how to load the room impulse response (RIR) from a wav-file, we will use for creating an equalization filter in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rir = pf.io.read_audio(os.path.join(\n",
    "    '..', '..', 'resources', 'impulse_response_measurement.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = pf.plot.time(rir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digital Signal Processing\n",
    "\n",
    "Typically, it is not feasible to equalize for the exact frequency response a room, as notches and peaks are sensitive to local variations of the receiver position.\n",
    "A more sensible approach is using a **smoothed magnitude spectrum** for room equalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd octave smoothing of the RIR\n",
    "rir_smoothed = pf.dsp.smooth_fractional_octave(rir, 3)[0]\n",
    "\n",
    "# inspect original and smoothed RTF\n",
    "plt.figure()\n",
    "ax = pf.plot.freq(rir, dB=True, label='Original RTF', color='grey')\n",
    "pf.plot.freq(rir_smoothed, dB=True, label='Smoothed RTF')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FIR equalization filter can be generated from the inverse magnitude spectrum in a straight forward manner. Due to the roll-off below 80 Hz, and above about 18 kHz, regularization outside these limits is required to avoid excessive noise amplification. These frequency limits are passed to the **inversion function**. Outside the desired frequency range, a regularization value is chosen such that the resulting spectrum does not show roll-offs, steep slopes, ringing or a long filter decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_range = [100, 18e3]\n",
    "rir_inv = pf.dsp.regularized_spectrum_inversion(rir_smoothed, frequency_range, regu_outside=5e-2)\n",
    "\n",
    "# inspect smoothed and inverted RTF\n",
    "plt.figure()\n",
    "ax = pf.plot.freq(rir_smoothed, dB=True, label='Smoothed RTF', color='grey')\n",
    "pf.plot.freq(rir_inv, dB=True, label='Inverted RTF')\n",
    "# indicate frequency range of inversion by dashed vertical lines\n",
    "ax.axvline(frequency_range[0], color='black', linestyle='--')\n",
    "ax.axvline(frequency_range[1], color='black', linestyle='--')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the smoothing returns a zero-phase filter, the phase response needs to be reconstructed to achieve causality. This is here achieved by using **minimum phase**.\n",
    "To guarantee no amplitude distortions of signals, the filter is further **normalized**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rir_inv = pf.dsp.normalize(rir_inv)\n",
    "rir_inv_min_phase = pf.dsp.minimum_phase(rir_inv)\n",
    "\n",
    "# inspect the minimum phase RIR with logarithmic y-axis\n",
    "plt.figure()\n",
    "pf.plot.time(rir_inv_min_phase, dB=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a **time window** is applied to crop the RIR to a typical FIR equalization filter length of 1024 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply time window and inspect the result\n",
    "equalization = pf.dsp.time_window(rir_inv_min_phase, [900, 1023], shape='right', crop='end', unit='samples')\n",
    "\n",
    "plt.figure()\n",
    "pf.plot.time(equalization, dB=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For validation of the created equalization filter, one may convolve it with the measured and unmodified room impulse response.\n",
    "Pyfar implements a dedicated **convolution function** supporting linear or cyclic convolution. \n",
    "However, the `*` operator may also be used for frequency domain multiplications; that is cyclic convolution in the time domain. Note that this operation requires matching signal lengths, so **zero-padding** is applied to the equalization filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applied_filter  = rir * pf.dsp.pad_zeros(equalization, rir.n_samples-equalization.n_samples)\n",
    "\n",
    "# Compare original and equalized RTF\n",
    "plt.figure()\n",
    "pf.plot.freq(rir, color='grey', label='Original RTF')\n",
    "ax = pf.plot.freq(applied_filter, label='Equalized RTF')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arithmetic operations\n",
    "Analogously to `*`, the `/` operator implements a frequency domain division, the `@` operator implements a frequency domain matrix multiplication, and the `**` operator is the frequency domain power operator. `+` and `-` operators are available, too. These are domain agnostic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Classes\n",
    "\n",
    "Pyfar implements **filters** as encapsulated class objects, similar to the Signal class.\n",
    "Filter objects support storing sets of coefficients for FIR, IIR, as well as second order section IIR (SOS) filters.\n",
    "They further facilitate piece-wise processing of audio signals while storing the filter state.\n",
    "\n",
    "A filter class object storing the previously created equalization filter only requires initializing the respective `pyfar.FilterFIR` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_filter = pf.FilterFIR(equalization.time, equalization.sampling_rate)\n",
    "eq_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example files\n",
    "The [pyfar.signals.files](https://pyfar.readthedocs.io/en/stable/modules/pyfar.signals.files.html) module provides signals such as speech, castanets, drums, and guitar snippets, e.g., to have more natural signals for a quick demo of an audio system. Here, we choose a short anechoic guitar sample to simulate playback at the measured listener position by convolving it with the RIR. Because the sampling rates of the guitar sample and RIR do not match, the guitar signal is **resampled** first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guitar = pf.signals.files.guitar()\n",
    "guitar = pf.dsp.resample(guitar, rir.sampling_rate)\n",
    "reverberant_guitar = pf.dsp.convolve(guitar, rir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equalization filter is applied to the guitar sample using the filter objects `process` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverberant_guitar_equalized = eq_filter.process(reverberant_guitar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both audio signals can be auralized using jupyter's Audio widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(reverberant_guitar.time, rate=guitar.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(reverberant_guitar_equalized.time, rate=guitar.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyfar implements a number of digital filters with various use-cases. For these filters, convenience functions exist as well, combining creation and application of a filter to a signal into a single function call.\n",
    "\n",
    "The following example applies a **fractional octave filter bank** to the RIR, creating a **multichannel signal**. For better visibility, use  `,`/`.`(in Windows) or `[` and `]` (Mac) to cycle through the channels inside the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply third octave filter in the frequency range from 250 Hz to 8 kHz\n",
    "rir_fractional_octaves = pf.dsp.filter.fractional_octave_bands(rir, 3, freq_range=[250, 8e3])\n",
    "\n",
    "plt.figure()\n",
    "pf.plot.freq(rir_fractional_octaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: HRTF\n",
    "As another example for a multichannel signal, let's load an head-related impulse responses (HRIR) dataset, its frequency domain equivalent is called head-related transfer function (HRTF). These signals contain the information how an incoming sound at the two ears is altered by the outer ear (pinna), head, and torso depending on its direction of incidence, which allows humans to localize sound. Example HRIRs are provided by the [pyfar.signals.files](https://pyfar.readthedocs.io/en/stable/modules/pyfar.signals.files.html) module.\n",
    "\n",
    "Use `,`/`.`(in Windows) or `[` and `]` (Mac) to cycle through the channels inside the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an example HRIR dataset on the horizontal plane\n",
    "hrirs, sources = pf.signals.files.head_related_impulse_responses(position='horizontal')\n",
    "\n",
    "plt.figure()\n",
    "pf.plot.time(hrirs)\n",
    "print(f\"hrirs.cshape={hrirs.cshape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As indicated by the property `cshape`, the `pyfar.Signal` object `hrirs` contains data for 180 source positions and both ears.\n",
    "\n",
    "## Coordinates\n",
    "\n",
    "The position of the sources were returned as the `pyfar.Coordinates` object `sources`. Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some information\n",
    "print(sources)\n",
    "# plot the points\n",
    "sources.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pyfar.Coordinates` class is designed for storing, manipulating, and accessing coordinate points. It supports a large variety of different coordinate conventions and the conversion between them. Visit the [coordinate concepts](https://pyfar.readthedocs.io/en/latest/concepts/pyfar.coordinates.html) for a graphical overview of available coordinate systems. Let's extract the azimuth angle in degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azimuth_deg = sources.azimuth * 180 / np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the left channel from the HRIRs using **slicing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = hrirs[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Plots\n",
    "**2D plotting** enables a different way of illustrating the audio data compared to the line plots from above, especially suitable for multichannel signals such as HRIRs.\n",
    "\n",
    "Note that the same shortcuts as before can be used to switch between different 2D plots. Toggling between the line and 2D plots is possible with `<`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-axis: azimuth in degree\n",
    "plt.figure()\n",
    "ax, _, _ = pf.plot.freq_2d(hrirs[:, 0], indices=azimuth_deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Packages\n",
    "Now, feel free to have a look at [pyfar.org](https://pyfar.org) to get an idea of the pyfar subpackages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License notice\n",
    "This notebook © 2024 by [the pyfar developers](https://github.com/orgs/pyfar/people) is licensed under [CC BY 4.0](http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1)\n",
    "\n",
    "![CC BY Large](https://mirrors.creativecommons.org/presskit/buttons/88x31/svg/by.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -m -iv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyfar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31fda18d23bccfca635c34b8e2ae5cca9cc1e70ba24d7dd63263c3d313ebed7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
