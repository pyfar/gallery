{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with pyfar Audio objects\n",
    "\n",
    "Audio data are at the core of pyfar. Three classes can be used for storing, processing, and visualizing such data. These are:\n",
    "\n",
    "- `pyfar.Signal` can be used to store equidistant and complete signals that can be converted between the time and frequency domain.\n",
    "- `pyfar.TimeData` and `pyfar.FrequencyData` are intended for data that cannot be converted between the time and frequency domain. Examples for this are time data with missing data and frequency data that is only available at certain frequencies, e.g., third-octave data.\n",
    "\n",
    "The following examples introduce fundamental concepts behind audio classes and show how the data inside audio objects can be accessed. See the `pyfar documentation` for a complete overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyfar as pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyfar Signals\n",
    "\n",
    "*Signals* are the most versatile and frequently used audio objects and are thus covered first. In addition, many concepts of *Signals* cary over to *TimeData* and *FrequencyData*.\n",
    "\n",
    "### Creating Signals\n",
    "\n",
    "A *Signal* can be created in the time domain by providing the time data and sampling rate in Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = pf.Signal([1, 0, 0, 0], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a *Signal* in the frequency domain requires to explicitly specify the length of the corresponding time signal in samples and the domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal2 = pf.Signal([1, 1, 1], 4, n_samples=4, domain='freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing data in Signals\n",
    "\n",
    "The time domain data stored in audio objects can be accessed trough their `time` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the times in seconds at which the data was samples is contained in the `times` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly the frequency domain data is stored in the `freq` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the corresponding frequencies in Hz in the `frequencies` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few important things to note right away.\n",
    "\n",
    "- data inside audio objects are stored in `numpy` arrays with at least 2-dimensions. In this example `signal` is an audio object with a single channel of audio data and the shape of `signal.time` is `(1, 4)`. In pyfar the samples of time data (4 in this example) and the frequency bins of frequency data are always stored in the last dimension of `signal.time` and `signal.freq`\n",
    "- The conversion between the time and frequency domain automatically happens upon request by the underlying *Signal* class. Internally the data is always stored in the domain that was last used and converted using the Discrete Fourier Transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also **important to note** that the data returned by the `freq` property can depend on the normalization of the Discrete Fourier Transform as explained in more detail `here`. The frequency data without any normalization can be accessed using the `freq_raw` property. However, for this example the two return the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.freq_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read on to learn how to find out which normalization of the Discrete Fourier Transform is used for a signal.\n",
    "\n",
    "The indices of a certain frequency or frequency range an be found using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = signal.find_nearest_frequency(1)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and in analogy `signal.find_nearest_time` can be used to find a certain point in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write data to Signals\n",
    "\n",
    "The same properties can be used to set the data inside audio objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.time = [2, 0, 0, 0]\n",
    "print(f'signal.time = {signal.time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal2.freq = [2, 2, 2]\n",
    "print(f'signal2.freq = {signal2.freq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional information stored in Signals\n",
    "\n",
    "Converting between time and frequency data is already useful, but audio objects can do more. They contain additional information about their data that can be helpful. Some basic information is already returned when prompting an audio object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets break down the information given by the *Signal* one by one. \n",
    "\n",
    "**Domain:** The signal is in the time domain, because we last accessed its time data above. We can get and set this property with the `signal.domain` attribute but it is of not much interest when working with Signal objects because they are automatically converted between the time and frequency domain, when it is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Signal Type:** It is an *energy* Signal, which is important to note and understand. Energy signals are finite signals with finite energy. Examples for these kinds of signals are impulse responses. *Power* signals on the other hand have an infinite duration and energy. Examples are noise or sine signals of which we typically observe a block of finite size. The signal type is a read only property that is set by the DFT normalization introduced below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.signal_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DFT Normalization:** For *power* Signals, different DFT normalization can be used that differently scale the values stored in `signal.freq`. For *energy* signals, the DFT normalization is always 'none'. For more information refer to the `Fast Fourier Transform` examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.fft_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal cshape, length, and caxis\n",
    "\n",
    "Signals can contain multiple channels of audio data and the shape of the data inside the audio object is often important. There are multiple attributes describing this.\n",
    "\n",
    "**Channel shape:** The *channel shape*, in short `cshape` gives the shape of the data inside a *Signal* but ignores the number of samples or frequency bins. For example our first signal created above has one channel and 4 samples, hence the data is of shape `(1, 4)`. Because the `cshape` ignores the samples it is `(1, )`. There are two ideas behind this. First, digital signal processing methods often work on channels, and second the length of the signal might be different in the time and frequency domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.cshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Length:** The signal length in samples and bins is stored in the properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.n_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that only the half sided spectrum is stored for real valued time signals. Hence the length differs in the time and frequency domain in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Channel axis:** Some functions in pyfar operate along one or more axes of the data. In analogy, to the channel shape, these axes are referred to as *channel axis* or in short `caxis`. Consider a signal of `cshape=(3, 4)` and a duration of `n_samples=128`. In this case `caxis=0` refers to the first dimension of size 3 and `caxis=-1` refers to the last dimension of size 4 but *not* the dimension containing the 128 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Channel Signals\n",
    "\n",
    "*Signals* may contain data in multiple channels and pyfar offers some functionality to ease handling such signals. Let the following signal of `cshape=(2, 3)` symbolize data obtained for 2 loudspeaker and 3 microphone positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal3 = pf.Signal(\n",
    "    [[[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0]],\n",
    "     [[0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]]],\n",
    "    6)\n",
    "\n",
    "signal3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subset of the channels can be accessed through slicing. For example the first channel in both dimensions can be obtained by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = signal3[0, 0]\n",
    "channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the output shows that the sloce is *Signal* as well, which is often useful because most pyfar functions require *Signals* as input. This also works for obtaining larger slices, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = signal3[0]\n",
    "channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "returns the data for the first loudspeaker and all three microphone positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyfar TimeData and FrequencyData\n",
    "\n",
    "To create *TimeData*, specify the data and the times at which the data was sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time data with non-equidistant sampling times of 0, 1, and 3 seconds\n",
    "time = pf.TimeData([1, 0, -1], [0, 1, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and creating *FrequencyData* requires the specification of the frequencies that belong to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency data containing values at 400, 800, and 1600 Hz\n",
    "frequency = pf.FrequencyData([1, .8, .7], [400, 800, 1600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This showed that creating *TimeData* and *FrequencyData* is different from creating *Signals*. The differences between the audio objects are also reflected in their print:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shows that *TimeData* and *FrequencyData* do not have a sampling rate and FFT normalization. Apart from this the different audio objects behave very similar. In fact the `pyfar.Signal` class is derived from `pyfar.TimeData` and `pyfar.FrequencyData`. Hence, the `time` property can be used to access *TimeData*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and the `freq` property can be used to access *FrequencyData*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency.freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, all time domain functionality of *Signals* that was introduced above is available for *TimeData* and all frequency domain functionality is available for *FrequencyData*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "After getting used to adding and accessing data in audio objects, you are ready for discovering ways of inspecting and working with audio objects. Good next steps might be learning how to graphically display (plot) data from audio objects as detailed `here` and check out how to apply simple arithmetic operations with audio objects as described `here`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyfar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
